# Исправления производительности v3.1

## Проблемы v3.0

1. **MAX_WORKERS_IO = 32** - слишком много потоков, вызывало конкуренцию за ресурсы
2. **Параллелизация JSON для всех данных** - накладные расходы превышали выигрыш для небольших данных
3. **Неправильный размер chunk** - создавалось слишком много маленьких chunks
4. **Синтаксические ошибки** - остатки старого кода после замены

## Исправления v3.1

### 1. Снижен MAX_WORKERS_IO
- **Было:** 32 потока
- **Стало:** 16 потоков
- **Эффект:** Меньше конкуренции за ресурсы, лучше производительность

### 2. Умная параллелизация JSON
- **Добавлена проверка размера:** Параллелизация только для данных >5000 строк
- **Оптимизирован размер chunk:** Минимум 2000 строк на chunk (вместо 1000)
- **Эффект:** Для небольших данных используется последовательная обработка (быстрее)

### 3. Исправлены синтаксические ошибки
- Удален дублирующий код
- Исправлена структура if-else

## Ожидаемые результаты

- **v3.0:** 23.486s - 25.545s
- **v3.1 (ожидается):** < 22.028s (быстрее v2)

## Изменения в коде

```python
# Было:
MAX_WORKERS_IO = min(32, (os.cpu_count() or 1) * 4)
chunk_size = max(1000, n_rows // (MAX_WORKERS_IO * 2))

# Стало:
MAX_WORKERS_IO = min(16, (os.cpu_count() or 1) * 2)
PARALLEL_JSON_THRESHOLD = 5000
if n_rows > PARALLEL_JSON_THRESHOLD:
    chunk_size = max(2000, n_rows // MAX_WORKERS_IO)
    # параллелизация
else:
    # последовательная обработка (быстрее для небольших данных)
```

## Рекомендации для тестирования

1. Запустить программу и сравнить время с v2 (22.028s)
2. Проверить что все функции работают корректно
3. Убедиться что нет ошибок в логах
